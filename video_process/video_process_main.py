"""
YOLOv8 + ByteTrack è¡Œäººå’Œè½¦è¾†æ£€æµ‹ä¸è¿½è¸ªç¨‹åº (å¸¦é€Ÿåº¦è®¡ç®—ã€æ‘„åƒå¤´è¿åŠ¨è¡¥å¿å’Œæ‹¥å µæ£€æµ‹)
æ”¹è¿›ï¼š
1. ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹ (yolov8l) æ”¹å–„è¿œè·ç¦»æ£€æµ‹
2. ä½¿ç”¨ByteTrackè¿½è¸ªï¼Œé¿å…é‡å¤è®¡æ•°
3. æ¯ä¸ªå¯¹è±¡åªè®¡æ•°ä¸€æ¬¡ï¼ˆé€šè¿‡å”¯ä¸€çš„track_idï¼‰
4. è®¡ç®—è¡Œäººå’Œè½¦è¾†çš„å®æ—¶é€Ÿåº¦
5. æ£€æµ‹æ‘„åƒå¤´è¿åŠ¨ï¼Œè‡ªåŠ¨è¡¥å¿
6. âœ… æ£€æµ‹é€Ÿåº¦æ€¥å‰§ä¸‹é™ï¼Œåˆ¤æ–­é“è·¯æ‹¥å µç­‰çº§
"""

import os
import cv2
import numpy as np
from pathlib import Path
import json
from datetime import datetime
import logging
from tqdm import tqdm
from ultralytics import YOLO
from collections import defaultdict
import math
from enum import Enum
from PIL import Image, ImageDraw, ImageFont

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class CongestionLevel(Enum):
    """é“è·¯æ‹¥å µç­‰çº§æšä¸¾"""
    SMOOTH = "smooth"          # ç•…é€š (ç»¿è‰²)
    LIGHT = "light"            # è½»å¾®æ‹¥å µ (é»„è‰²)
    MODERATE = "moderate"      # ä¸­ç­‰æ‹¥å µ (æ©™è‰²)
    HEAVY = "heavy"            # ä¸¥é‡æ‹¥å µ (çº¢è‰²)
    SEVERE = "severe"          # æåº¦æ‹¥å µ (æ·±çº¢è‰²)


class CongestionDetector:
    """é“è·¯æ‹¥å µæ£€æµ‹å™¨"""
    
    def __init__(self, 
                 window_size=30,
                 speed_drop_threshold=0.4,
                 density_threshold=0.15):
        """
        åˆå§‹åŒ–æ‹¥å µæ£€æµ‹å™¨
        
        Args:
            window_size: æ—¶é—´çª—å£å¤§å°ï¼ˆå¸§æ•°ï¼‰
            speed_drop_threshold: é€Ÿåº¦ä¸‹é™é˜ˆå€¼æ¯”ä¾‹ (0-1)
            density_threshold: å¯†åº¦é˜ˆå€¼ (0-1)
        """
        self.window_size = window_size
        self.speed_drop_threshold = speed_drop_threshold
        self.density_threshold = density_threshold
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.speed_history = defaultdict(list)  # æ¯ä¸ªå¯¹è±¡çš„é€Ÿåº¦å†å²
        self.object_count_history = []  # æ¯å¸§çš„å¯¹è±¡æ•°é‡
        self.speed_drop_events = []  # é€Ÿåº¦æ€¥å‰§ä¸‹é™äº‹ä»¶
        self.frame_congestion_levels = []  # æ¯å¸§çš„æ‹¥å µç­‰çº§
    
    def calculate_speed_drop_ratio(self, speeds):
        """
        è®¡ç®—é€Ÿåº¦ä¸‹é™æ¯”ä¾‹
        
        Args:
            speeds: é€Ÿåº¦åˆ—è¡¨ (æœ€è¿‘çš„çª—å£å†…çš„é€Ÿåº¦)
        
        Returns:
            (ä¸‹é™æ¯”ä¾‹, å¹³å‡é€Ÿåº¦)
        """
        if len(speeds) < 2:
            return 0, 0
        
        # ä½¿ç”¨å‰åŠæ®µå’ŒååŠæ®µçš„å¹³å‡é€Ÿåº¦æ¯”è¾ƒ
        mid = len(speeds) // 2
        avg_first_half = np.mean(speeds[:mid]) if mid > 0 else speeds[0]
        avg_second_half = np.mean(speeds[mid:])
        
        if avg_first_half > 0:
            drop_ratio = (avg_first_half - avg_second_half) / avg_first_half
        else:
            drop_ratio = 0
        
        return max(0, drop_ratio), avg_second_half
    
    def calculate_traffic_density(self, current_object_count, frame_area):
        """
        è®¡ç®—äº¤é€šå¯†åº¦
        
        Args:
            current_object_count: å½“å‰å¸§çš„å¯¹è±¡æ•°é‡
            frame_area: å¸§çš„åƒç´ é¢ç§¯
        
        Returns:
            å¯†åº¦å€¼ (0-1)
        """
        if frame_area == 0:
            return 0
        
        # å¯†åº¦ = å¯¹è±¡æ•°é‡ / (å¸§é¢ç§¯ / æ ‡å‡†åŒ–ç³»æ•°)
        # æ ‡å‡†åŒ–ç³»æ•°ä½¿å¯†åº¦åœ¨åˆç†èŒƒå›´å†…
        normalization_factor = frame_area / 100000  # 100000åƒç´ ä¸ºåŸºå‡†
        density = min(1.0, (current_object_count / max(1, normalization_factor)))
        
        return density
    
    def detect_congestion_level(self, speeds, object_count, frame_area):
        """
        æ£€æµ‹é“è·¯æ‹¥å µç­‰çº§
        
        Args:
            speeds: æœ€è¿‘çš„é€Ÿåº¦æ•°æ®
            object_count: å½“å‰äº¤é€šå¯¹è±¡æ•°é‡
            frame_area: å¸§çš„åƒç´ é¢ç§¯
        
        Returns:
            (æ‹¥å µç­‰çº§, è¯¦ç»†ä¿¡æ¯å­—å…¸)
        """
        details = {
            'avg_speed': 0,
            'speed_drop_ratio': 0,
            'traffic_density': 0,
            'congestion_factors': []
        }
        
        # è®¡ç®—å¹³å‡é€Ÿåº¦å’Œé€Ÿåº¦ä¸‹é™
        if speeds:
            details['avg_speed'] = np.mean(speeds)
            drop_ratio, _ = self.calculate_speed_drop_ratio(speeds)
            details['speed_drop_ratio'] = drop_ratio
        
        # è®¡ç®—äº¤é€šå¯†åº¦
        density = self.calculate_traffic_density(object_count, frame_area)
        details['traffic_density'] = density
        
        # åˆ¤æ–­æ‹¥å µç­‰çº§çš„å› ç´ 
        congestion_score = 0
        
        # 1. é€Ÿåº¦å› ç´  (æƒé‡: 40%)
        if details['avg_speed'] < 1.0:  # ä½äº3.6 km/h
            congestion_score += 0.4
            details['congestion_factors'].append('æä½é€Ÿåº¦')
        elif details['avg_speed'] < 2.5:  # ä½äº9 km/h
            congestion_score += 0.3
            details['congestion_factors'].append('ä½é€Ÿåº¦')
        elif details['avg_speed'] < 5.0:  # ä½äº18 km/h
            congestion_score += 0.15
            details['congestion_factors'].append('ä¸­ç­‰é€Ÿåº¦')
        
        # 2. é€Ÿåº¦ä¸‹é™å› ç´  (æƒé‡: 30%)
        if details['speed_drop_ratio'] > 0.5:  # é€Ÿåº¦ä¸‹é™è¶…è¿‡50%
            congestion_score += 0.3
            details['congestion_factors'].append('é€Ÿåº¦æ€¥å‰§ä¸‹é™')
        elif details['speed_drop_ratio'] > 0.3:  # é€Ÿåº¦ä¸‹é™è¶…è¿‡30%
            congestion_score += 0.2
            details['congestion_factors'].append('é€Ÿåº¦æ˜¾è‘—ä¸‹é™')
        elif details['speed_drop_ratio'] > 0.1:  # é€Ÿåº¦ä¸‹é™è¶…è¿‡10%
            congestion_score += 0.1
            details['congestion_factors'].append('é€Ÿåº¦ç•¥æœ‰ä¸‹é™')
        
        # 3. å¯†åº¦å› ç´  (æƒé‡: 30%)
        if density > 0.8:  # å¯†åº¦éå¸¸é«˜
            congestion_score += 0.3
            details['congestion_factors'].append('äº¤é€šå¯†åº¦æé«˜')
        elif density > 0.5:  # å¯†åº¦å¾ˆé«˜
            congestion_score += 0.2
            details['congestion_factors'].append('äº¤é€šå¯†åº¦å¾ˆé«˜')
        elif density > self.density_threshold:  # å¯†åº¦è¾ƒé«˜
            congestion_score += 0.1
            details['congestion_factors'].append('äº¤é€šå¯†åº¦è¾ƒé«˜')
        
        # æ ¹æ®ç»¼åˆå¾—åˆ†åˆ¤æ–­æ‹¥å µç­‰çº§
        if congestion_score < 0.15:
            level = CongestionLevel.SMOOTH
        elif congestion_score < 0.35:
            level = CongestionLevel.LIGHT
        elif congestion_score < 0.55:
            level = CongestionLevel.MODERATE
        elif congestion_score < 0.75:
            level = CongestionLevel.HEAVY
        else:
            level = CongestionLevel.SEVERE
        
        details['congestion_score'] = congestion_score
        details['level'] = level.value
        
        return level, details


class CameraMotionDetector:
    """æ‘„åƒå¤´è¿åŠ¨æ£€æµ‹å™¨ - ä½¿ç”¨ç‰¹å¾ç‚¹è¿½è¸ª"""
    
    def __init__(self, max_corners=200, quality_level=0.01, min_distance=10):
        """
        åˆå§‹åŒ–æ‘„åƒå¤´è¿åŠ¨æ£€æµ‹å™¨
        
        Args:
            max_corners: æœ€å¤šæ£€æµ‹çš„è§’ç‚¹æ•°
            quality_level: è§’ç‚¹è´¨é‡é˜ˆå€¼
            min_distance: è§’ç‚¹ä¹‹é—´çš„æœ€å°è·ç¦»
        """
        self.max_corners = max_corners
        self.quality_level = quality_level
        self.min_distance = min_distance
        self.prev_gray = None
        self.prev_points = None
    
    def detect_motion(self, frame):
        """
        æ£€æµ‹æ‘„åƒå¤´è¿åŠ¨
        
        Returns:
            (camera_dx, camera_dy) - æ‘„åƒå¤´åœ¨xå’Œyæ–¹å‘çš„åƒç´ ç§»åŠ¨é‡
        """
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        camera_dx, camera_dy = 0, 0
        
        # æ£€æµ‹è§’ç‚¹
        corners = cv2.goodFeaturesToTrack(
            gray, 
            maxCorners=self.max_corners,
            qualityLevel=self.quality_level,
            minDistance=self.min_distance,
            blockSize=7,
            useHarrisDetector=False
        )
        
        if corners is not None and self.prev_points is not None:
            try:
                # ä½¿ç”¨Lucas-Kanadeå…‰æµç®—æ³•è¿½è¸ªç‰¹å¾ç‚¹
                next_points, status, err = cv2.calcOpticalFlowPyrLK(
                    self.prev_gray, gray, self.prev_points, None,
                    winSize=(15, 15),
                    maxLevel=3,
                    criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03)
                )
                
                # ç­›é€‰å‡ºå¥½çš„è¿½è¸ªç‚¹
                if status is not None:
                    good_old = self.prev_points[status == 1]
                    good_new = next_points[status == 1]
                    
                    if len(good_old) > 10:  # è‡³å°‘è¦æœ‰10ä¸ªå¯é çš„è¿½è¸ªç‚¹
                        # è®¡ç®—æ‰€æœ‰ç‚¹çš„å¹³å‡è¿åŠ¨
                        movements = good_new - good_old
                        camera_dx = np.median(movements[:, 0])
                        camera_dy = np.median(movements[:, 1])
            except Exception as e:
                logger.debug(f"å…‰æµè®¡ç®—å‡ºé”™: {e}")
                camera_dx, camera_dy = 0, 0
        
        # ä¿å­˜å½“å‰å¸§çš„ä¿¡æ¯ç”¨äºä¸‹ä¸€å¸§
        self.prev_gray = gray
        self.prev_points = corners
        
        return camera_dx, camera_dy


class YOLOTrackerDetector:
    """YOLOv8 + ByteTrack è¡Œäººå’Œè½¦è¾†æ£€æµ‹è¿½è¸ªå™¨ï¼ˆå¸¦æ‹¥å µæ£€æµ‹ï¼‰"""
    
    # COCOæ•°æ®é›†ä¸­çš„ç±»åˆ«IDæ˜ å°„
    CLASS_NAMES = {
        0: 'person',      # è¡Œäºº
        1: 'bicycle',     # è‡ªè¡Œè½¦
        2: 'car',         # å°è½¿è½¦
        3: 'motorcycle',  # æ‘©æ‰˜è½¦
        5: 'bus',         # å…¬å…±æ±½è½¦
        7: 'truck',       # å¡è½¦
    }
    
    # ç±»åˆ«åˆ°æˆ‘ä»¬éœ€è¦çš„ç±»åˆ«çš„æ˜ å°„
    CLASS_MAPPING = {
        'person': 'pedestrian',        # è¡Œäºº
        'bicycle': 'bicycle',          # è‡ªè¡Œè½¦
        'car': 'car',                  # å°è½¿è½¦
        'motorcycle': 'bicycle',       # æ‘©æ‰˜è½¦å½’ä¸ºè‡ªè¡Œè½¦
        'bus': 'bus',                  # å…¬å…±æ±½è½¦
        'truck': 'car',                # å¡è½¦å½’ä¸ºå°è½¿è½¦
    }
    
    # é¢œè‰²é…ç½® (BGR)
    COLORS = {
        'pedestrian': (0, 255, 0),     # ç»¿è‰²
        'bicycle': (255, 0, 0),        # è“è‰²
        'car': (0, 0, 255),            # çº¢è‰²
        'bus': (0, 165, 255),          # æ©™è‰²
    }
    
    # æ‹¥å µç­‰çº§é¢œè‰²é…ç½®
    CONGESTION_COLORS = {
        CongestionLevel.SMOOTH: (0, 255, 0),      # ç»¿è‰²
        CongestionLevel.LIGHT: (0, 255, 255),     # é»„è‰²
        CongestionLevel.MODERATE: (0, 165, 255),  # æ©™è‰²
        CongestionLevel.HEAVY: (0, 0, 255),       # çº¢è‰²
        CongestionLevel.SEVERE: (0, 0, 139),      # æ·±çº¢è‰²
    }
    
    # æ‹¥å µç­‰çº§ä¸­æ–‡åç§°
    CONGESTION_NAMES_CN = {
        CongestionLevel.SMOOTH: 'ç•…é€š',
        CongestionLevel.LIGHT: 'è½»å¾®æ‹¥å µ',
        CongestionLevel.MODERATE: 'ä¸­ç­‰æ‹¥å µ',
        CongestionLevel.HEAVY: 'ä¸¥é‡æ‹¥å µ',
        CongestionLevel.SEVERE: 'æåº¦æ‹¥å µ',
    }
    
    # æ‹¥å µç­‰çº§è‹±æ–‡åç§°ï¼ˆå¤‡ç”¨ï¼‰
    CONGESTION_NAMES_EN = {
        CongestionLevel.SMOOTH: 'Smooth',
        CongestionLevel.LIGHT: 'Light Congestion',
        CongestionLevel.MODERATE: 'Moderate Congestion',
        CongestionLevel.HEAVY: 'Heavy Congestion',
        CongestionLevel.SEVERE: 'Severe Congestion',
    }
    
    def __init__(self, 
                 model_name='yolov8l.pt',
                 conf_threshold=0.3,
                 iou_threshold=0.5,
                 output_dir='output_video',
                 input_dir='pre_video',
                 tracker='bytetrack.yaml',
                 pixels_per_meter=50,
                 enable_camera_compensation=True,
                 enable_congestion_detection=True,
                 use_chinese_text=True):
        """
        åˆå§‹åŒ–æ£€æµ‹è¿½è¸ªå™¨
        
        Args:
            model_name: YOLOv8æ¨¡å‹åç§°
            conf_threshold: ç½®ä¿¡åº¦é˜ˆå€¼
            iou_threshold: NMS IOUé˜ˆå€¼
            output_dir: è¾“å‡ºç›®å½•
            input_dir: è¾“å…¥è§†é¢‘ç›®å½•
            tracker: è¿½è¸ªç®—æ³•
            pixels_per_meter: åƒç´ ä¸ç±³çš„è½¬æ¢æ¯”ä¾‹
            enable_camera_compensation: æ˜¯å¦å¯ç”¨æ‘„åƒå¤´è¿åŠ¨è¡¥å¿
            enable_congestion_detection: æ˜¯å¦å¯ç”¨æ‹¥å µæ£€æµ‹
            use_chinese_text: æ˜¯å¦ä½¿ç”¨ä¸­æ–‡æ–‡æœ¬æ˜¾ç¤º
        """
        self.model_name = model_name
        self.conf_threshold = conf_threshold
        self.iou_threshold = iou_threshold
        self.output_dir = output_dir
        self.input_dir = input_dir
        self.tracker_type = tracker
        self.pixels_per_meter = pixels_per_meter
        self.enable_camera_compensation = enable_camera_compensation
        self.enable_congestion_detection = enable_congestion_detection
        self.use_chinese_text = use_chinese_text
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        Path(self.output_dir).mkdir(parents=True, exist_ok=True)
        Path(f"{self.output_dir}/stats").mkdir(parents=True, exist_ok=True)
        
        # åŠ è½½æ¨¡å‹
        logger.info(f"åŠ è½½YOLOv8æ¨¡å‹: {model_name}")
        self.model = YOLO(model_name)
        logger.info("æ¨¡å‹åŠ è½½å®Œæˆ")
        
        # åˆå§‹åŒ–æ‘„åƒå¤´è¿åŠ¨æ£€æµ‹å™¨
        self.camera_motion_detector = CameraMotionDetector()
        
        # åˆå§‹åŒ–æ‹¥å µæ£€æµ‹å™¨
        self.congestion_detector = CongestionDetector(
            window_size=30,
            speed_drop_threshold=0.4,
            density_threshold=0.15
        )
        
        # å°è¯•åŠ è½½ä¸­æ–‡å­—ä½“
        self.font_path = None
        if self.use_chinese_text:
            # å¸¸è§çš„ä¸­æ–‡å­—ä½“è·¯å¾„
            possible_fonts = [
                # Windows
                "C:/Windows/Fonts/simhei.ttf",  # é»‘ä½“
                "C:/Windows/Fonts/simsun.ttc",  # å®‹ä½“
                "C:/Windows/Fonts/msyh.ttc",    # å¾®è½¯é›…é»‘
                # Linux
                "/usr/share/fonts/truetype/droid/DroidSansFallbackFull.ttf",
                "/usr/share/fonts/truetype/wqy/wqy-microhei.ttc",
                # macOS
                "/Library/Fonts/Arial Unicode.ttf",
                "/System/Library/Fonts/PingFang.ttc",
            ]
            
            for font_path in possible_fonts:
                if os.path.exists(font_path):
                    self.font_path = font_path
                    logger.info(f"æ‰¾åˆ°ä¸­æ–‡å­—ä½“: {font_path}")
                    break
            
            if self.font_path is None:
                logger.warning("æœªæ‰¾åˆ°ä¸­æ–‡å­—ä½“ï¼Œå°†ä½¿ç”¨è‹±æ–‡æ˜¾ç¤º")
                self.use_chinese_text = False
        
        # æ‘„åƒå¤´è¿åŠ¨ç»Ÿè®¡
        self.camera_stats = {
            'motions': [],
            'total_motion': (0, 0),
        }
        
        # è¿½è¸ªç»Ÿè®¡ä¿¡æ¯
        self.track_stats = {
            'total_frames': 0,
            'unique_pedestrians': set(),
            'unique_bicycles': set(),
            'unique_cars': set(),
            'unique_buses': set(),
            'track_history': defaultdict(list),
            'track_speeds_absolute': defaultdict(list),
            'track_speeds_relative': defaultdict(list),
            'frame_detections': [],
            'max_speeds_absolute': defaultdict(float),
            'max_speeds_relative': defaultdict(float),
            'avg_speeds_absolute': defaultdict(list),
            'avg_speeds_relative': defaultdict(list),
            'congestion_history': [],  # æ¯å¸§çš„æ‹¥å µä¿¡æ¯
        }
    
    def get_class_label(self, class_id):
        """è·å–ç±»åˆ«æ ‡ç­¾"""
        if class_id in self.CLASS_NAMES:
            original_name = self.CLASS_NAMES[class_id]
            mapped_name = self.CLASS_MAPPING.get(original_name, original_name)
            return mapped_name
        return None
    
    def calculate_distance(self, p1, p2):
        """è®¡ç®—ä¸¤ç‚¹ä¹‹é—´çš„æ¬§å‡ é‡Œå¾—è·ç¦»ï¼ˆåƒç´ ï¼‰"""
        return math.sqrt((p2[0] - p1[0])**2 + (p2[1] - p1[1])**2)
    
    def calculate_speed(self, distance_pixels, fps, pixels_per_meter=50):
        """è®¡ç®—é€Ÿåº¦"""
        time_interval = 1.0 / fps
        distance_meters = distance_pixels / pixels_per_meter
        speed_ms = distance_meters / time_interval
        speed_kmh = speed_ms * 3.6
        return speed_ms, speed_kmh
    
    def compensate_camera_motion(self, object_speed_ms, camera_motion, fps, pixels_per_meter):
        """è¡¥å¿æ‘„åƒå¤´è¿åŠ¨ï¼Œè®¡ç®—å¯¹è±¡çš„ç›¸å¯¹é€Ÿåº¦"""
        camera_motion_pixels = math.sqrt(camera_motion[0]**2 + camera_motion[1]**2)
        camera_speed_ms, _ = self.calculate_speed(camera_motion_pixels, fps, pixels_per_meter)
        relative_speed_ms = object_speed_ms - camera_speed_ms
        return relative_speed_ms, camera_speed_ms
    
    def put_chinese_text(self, image, text, position, font_size=20, color=(255, 255, 255)):
        """
        åœ¨å›¾åƒä¸Šç»˜åˆ¶ä¸­æ–‡æ–‡æœ¬
        
        Args:
            image: OpenCVå›¾åƒ (BGRæ ¼å¼)
            text: è¦ç»˜åˆ¶çš„ä¸­æ–‡æ–‡æœ¬
            position: æ–‡æœ¬ä½ç½® (x, y)
            font_size: å­—ä½“å¤§å°
            color: æ–‡æœ¬é¢œè‰² (BGRæ ¼å¼)
        
        Returns:
            ç»˜åˆ¶æ–‡æœ¬åçš„å›¾åƒ
        """
        if not self.use_chinese_text or self.font_path is None:
            # ä½¿ç”¨è‹±æ–‡æ›¿ä»£
            english_text = self.CONGESTION_NAMES_EN.get(text, text)
            cv2.putText(image, english_text, position, cv2.FONT_HERSHEY_SIMPLEX, 
                       font_size/20, color, 2)
            return image
        
        try:
            # å°†OpenCVå›¾åƒè½¬æ¢ä¸ºPILå›¾åƒ
            pil_image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
            draw = ImageDraw.Draw(pil_image)
            
            # åŠ è½½ä¸­æ–‡å­—ä½“
            font = ImageFont.truetype(self.font_path, font_size)
            
            # ç»˜åˆ¶æ–‡æœ¬
            draw.text(position, text, font=font, fill=color)
            
            # å°†PILå›¾åƒè½¬æ¢å›OpenCVæ ¼å¼
            image = cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)
        except Exception as e:
            logger.warning(f"ç»˜åˆ¶ä¸­æ–‡æ–‡æœ¬å¤±è´¥: {e}, ä½¿ç”¨è‹±æ–‡æ›¿ä»£")
            english_text = self.CONGESTION_NAMES_EN.get(text, text)
            cv2.putText(image, english_text, position, cv2.FONT_HERSHEY_SIMPLEX, 
                       font_size/20, color, 2)
        
        return image
    
    def draw_detections_with_tracking(self, frame, results, fps, camera_motion):
        """åœ¨å¸§ä¸Šç»˜åˆ¶æ£€æµ‹ç»“æœã€è¿½è¸ªä¿¡æ¯å’Œé€Ÿåº¦"""
        frame_stats = {
            'pedestrian': [],
            'bicycle': [],
            'car': [],
            'bus': [],
            'detections': [],
            'camera_motion': camera_motion,
        }
        
        if results is None or len(results) == 0:
            return frame, frame_stats
        
        result = results[0]
        
        if result.boxes is not None and len(result.boxes) > 0:
            boxes = result.boxes
            
            for box in boxes:
                class_id = int(box.cls[0])
                conf = float(box.conf[0])
                track_id = None
                
                if hasattr(box, 'id') and box.id is not None:
                    track_id = int(box.id[0])
                
                if conf < self.conf_threshold:
                    continue
                
                class_label = self.get_class_label(class_id)
                if class_label is None:
                    continue
                
                if track_id is not None:
                    if class_label == 'pedestrian':
                        self.track_stats['unique_pedestrians'].add(track_id)
                    elif class_label == 'bicycle':
                        self.track_stats['unique_bicycles'].add(track_id)
                    elif class_label == 'car':
                        self.track_stats['unique_cars'].add(track_id)
                    elif class_label == 'bus':
                        self.track_stats['unique_buses'].add(track_id)
                
                x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)
                center = (int((x1 + x2) / 2), int((y1 + y2) / 2))
                
                speed_ms_absolute = 0
                speed_kmh_absolute = 0
                speed_ms_relative = 0
                speed_kmh_relative = 0
                camera_speed_ms = 0
                
                if track_id is not None:
                    history = self.track_stats['track_history'][track_id]
                    
                    if len(history) > 0:
                        last_position = history[-1]
                        distance_pixels = self.calculate_distance(last_position, center)
                        speed_ms_absolute, speed_kmh_absolute = self.calculate_speed(
                            distance_pixels, fps, self.pixels_per_meter
                        )
                        
                        if self.enable_camera_compensation:
                            speed_ms_relative, camera_speed_ms = self.compensate_camera_motion(
                                speed_ms_absolute, camera_motion, fps, self.pixels_per_meter
                            )
                            speed_kmh_relative = speed_ms_relative * 3.6
                        else:
                            speed_ms_relative = speed_ms_absolute
                            speed_kmh_relative = speed_kmh_absolute
                        
                        if speed_ms_absolute < 50:
                            self.track_stats['track_speeds_absolute'][track_id].append(speed_ms_absolute)
                            self.track_stats['track_speeds_relative'][track_id].append(speed_ms_relative)
                            self.track_stats['avg_speeds_absolute'][track_id].append(speed_ms_absolute)
                            self.track_stats['avg_speeds_relative'][track_id].append(speed_ms_relative)
                            
                            if speed_ms_absolute > self.track_stats['max_speeds_absolute'].get(track_id, 0):
                                self.track_stats['max_speeds_absolute'][track_id] = speed_ms_absolute
                            if speed_ms_relative > self.track_stats['max_speeds_relative'].get(track_id, 0):
                                self.track_stats['max_speeds_relative'][track_id] = speed_ms_relative
                    
                    self.track_stats['track_history'][track_id].append(center)
                    if len(self.track_stats['track_history'][track_id]) > 100:
                        self.track_stats['track_history'][track_id].pop(0)
                
                frame_stats[class_label].append({
                    'track_id': track_id,
                    'confidence': round(conf, 3),
                    'bbox': box.xyxy[0].cpu().numpy().tolist(),
                    'speed_ms_absolute': round(speed_ms_absolute, 2),
                    'speed_kmh_absolute': round(speed_kmh_absolute, 2),
                    'speed_ms_relative': round(speed_ms_relative, 2),
                    'speed_kmh_relative': round(speed_kmh_relative, 2),
                    'camera_speed_ms': round(camera_speed_ms, 2),
                    'center': center
                })
                
                color = self.COLORS.get(class_label, (255, 255, 255))
                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)
                
                if track_id is not None:
                    if self.enable_camera_compensation:
                        label_text = f"{class_label} (ID:{track_id}) {speed_kmh_relative:.1f}km/h"
                    else:
                        label_text = f"{class_label} (ID:{track_id}) {speed_kmh_absolute:.1f}km/h"
                else:
                    label_text = f"{class_label}: {conf:.2f}"
                
                label_size, baseline = cv2.getTextSize(
                    label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2
                )
                
                cv2.rectangle(
                    frame,
                    (x1, y1 - label_size[1] - baseline - 5),
                    (x1 + label_size[0], y1),
                    color,
                    -1
                )
                
                cv2.putText(
                    frame,
                    label_text,
                    (x1, y1 - baseline - 5),
                    cv2.FONT_HERSHEY_SIMPLEX,
                    0.6,
                    (255, 255, 255),
                    2
                )
                
                if track_id is not None:
                    points = self.track_stats['track_history'][track_id]
                    if len(points) > 1:
                        for i in range(1, len(points)):
                            cv2.line(frame, points[i-1], points[i], color, 2)
                    cv2.circle(frame, center, 4, color, -1)
        
        return frame, frame_stats
    
    def draw_statistics(self, frame, frame_stats, unique_stats, frame_number, fps, camera_motion, congestion_level, congestion_details):
        """åœ¨å¸§ä¸Šç»˜åˆ¶ç»Ÿè®¡ä¿¡æ¯å’Œæ‹¥å µç­‰çº§"""
        h, w = frame.shape[:2]
        
        # è·å–æ‹¥å µç­‰çº§é¢œè‰²
        congestion_color = self.CONGESTION_COLORS.get(congestion_level, (255, 255, 255))
        congestion_name_cn = self.CONGESTION_NAMES_CN.get(congestion_level, 'æœªçŸ¥')
        congestion_name_en = self.CONGESTION_NAMES_EN.get(congestion_level, 'Unknown')
        
        # 1. å·¦ä¸Šè§’ï¼šæ‹¥å µç­‰çº§å¤§æ˜¾ç¤º
        if self.enable_congestion_detection:
            overlay_congestion = frame.copy()
            cv2.rectangle(overlay_congestion, (10, 10), (400, 100), congestion_color, -1)
            cv2.addWeighted(overlay_congestion, 0.3, frame, 0.7, 0, frame)
            
            text_color = (255, 255, 255)
            
            # ä½¿ç”¨ä¸­æ–‡æˆ–è‹±æ–‡æ˜¾ç¤ºæ‹¥å µç­‰çº§
            if self.use_chinese_text:
                frame = self.put_chinese_text(frame, f"æ‹¥å µç­‰çº§: {congestion_name_cn}", (20, 40), 24, text_color)
            else:
                cv2.putText(frame, f"Congestion: {congestion_name_en}", (20, 40),
                           cv2.FONT_HERSHEY_SIMPLEX, 1.0, text_color, 3)
            
            level_text = f"Level: {congestion_details['congestion_score']:.2f}"
            cv2.putText(frame, level_text, (20, 75),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, text_color, 2)
        
        # 2. å·¦ä¸­ï¼šæ£€æµ‹ä¿¡æ¯
        overlay = frame.copy()
        cv2.rectangle(overlay, (10, 110), (450, 290), (0, 0, 0), -1)
        cv2.addWeighted(overlay, 0.3, frame, 0.7, 0, frame)
        
        text_color = (255, 255, 255)
        y_offset = 135
        
        current_time = frame_number / fps
        time_text = f"Time: {current_time:.1f}s"
        cv2.putText(frame, time_text, (20, y_offset),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, text_color, 2)
        
        camera_motion_pixels = math.sqrt(camera_motion[0]**2 + camera_motion[1]**2)
        camera_speed_ms, camera_speed_kmh = self.calculate_speed(
            camera_motion_pixels, fps, self.pixels_per_meter
        )
        camera_text = f"Camera: {camera_speed_kmh:.1f}km/h"
        cv2.putText(frame, camera_text, (20, y_offset + 25),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 165, 0), 2)
        
        y_offset += 50
        cv2.putText(frame, "Objects Detected:", (20, y_offset),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, text_color, 2)
        
        y_offset += 25
        cv2.putText(frame, f"Pedestrian: {len(frame_stats['pedestrian'])}", (20, y_offset),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, self.COLORS['pedestrian'], 2)
        
        y_offset += 20
        cv2.putText(frame, f"Bicycle: {len(frame_stats['bicycle'])}", (20, y_offset),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, self.COLORS['bicycle'], 2)
        
        y_offset += 20
        cv2.putText(frame, f"Car: {len(frame_stats['car'])}", (20, y_offset),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, self.COLORS['car'], 2)
        
        y_offset += 20
        cv2.putText(frame, f"Bus: {len(frame_stats['bus'])}", (20, y_offset),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, self.COLORS['bus'], 2)
        
        # 3. å³ä¸Šè§’ï¼šæ€»ä½“ç»Ÿè®¡
        overlay2 = frame.copy()
        cv2.rectangle(overlay2, (w - 500, 10), (w - 10, 290), (0, 0, 0), -1)
        cv2.addWeighted(overlay2, 0.3, frame, 0.7, 0, frame)
        
        y_offset = 35
        cv2.putText(frame, "Total Unique Count:", (w - 490, y_offset),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, text_color, 2)
        
        y_offset += 25
        cv2.putText(frame, f"Pedestrian: {len(unique_stats['pedestrians'])}", (w - 490, y_offset),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, self.COLORS['pedestrian'], 2)
        
        y_offset += 20
        cv2.putText(frame, f"Bicycle: {len(unique_stats['bicycles'])}", (w - 490, y_offset),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, self.COLORS['bicycle'], 2)
        
        y_offset += 20
        cv2.putText(frame, f"Car: {len(unique_stats['cars'])}", (w - 490, y_offset),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, self.COLORS['car'], 2)
        
        y_offset += 20
        cv2.putText(frame, f"Bus: {len(unique_stats['buses'])}", (w - 490, y_offset),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, self.COLORS['bus'], 2)
        
        # æ‹¥å µå› ç´ åˆ†æ
        y_offset += 30
        if self.use_chinese_text:
            frame = self.put_chinese_text(frame, "æ‹¥å µå› ç´ :", (w - 490, y_offset), 16, congestion_color)
        else:
            cv2.putText(frame, "Congestion Factors:", (w - 490, y_offset),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, congestion_color, 2)
        
        y_offset += 25
        for i, factor in enumerate(congestion_details['congestion_factors'][:3]):
            if self.use_chinese_text:
                frame = self.put_chinese_text(frame, f"â€¢ {factor}", (w - 480, y_offset + i*20), 12, congestion_color)
            else:
                # å°†ä¸­æ–‡å› ç´ ç¿»è¯‘ä¸ºè‹±æ–‡
                factor_en = {
                    'æä½é€Ÿåº¦': 'Very Low Speed',
                    'ä½é€Ÿåº¦': 'Low Speed',
                    'ä¸­ç­‰é€Ÿåº¦': 'Medium Speed',
                    'é€Ÿåº¦æ€¥å‰§ä¸‹é™': 'Sharp Speed Drop',
                    'é€Ÿåº¦æ˜¾è‘—ä¸‹é™': 'Significant Speed Drop',
                    'é€Ÿåº¦ç•¥æœ‰ä¸‹é™': 'Slight Speed Drop',
                    'äº¤é€šå¯†åº¦æé«˜': 'Extreme Density',
                    'äº¤é€šå¯†åº¦å¾ˆé«˜': 'High Density',
                    'äº¤é€šå¯†åº¦è¾ƒé«˜': 'Medium Density'
                }.get(factor, factor)
                cv2.putText(frame, f"â€¢ {factor_en}", (w - 480, y_offset + i*20),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.4, congestion_color, 1)
        
        # 4. ä¸‹æ–¹ï¼šé€Ÿåº¦å’Œå¯†åº¦ä¿¡æ¯
        overlay3 = frame.copy()
        cv2.rectangle(overlay3, (10, h - 120), (w - 10, h - 10), (0, 0, 0), -1)
        cv2.addWeighted(overlay3, 0.3, frame, 0.7, 0, frame)
        
        y_offset = h - 90
        if self.enable_camera_compensation:
            if self.use_chinese_text:
                frame = self.put_chinese_text(frame, "ç›¸å¯¹é€Ÿåº¦ (km/h) [å·²è¡¥å¿æ‘„åƒå¤´è¿åŠ¨]:", (20, y_offset), 16, text_color)
            else:
                cv2.putText(frame, "Relative Speed (km/h) [Camera Compensated]:", (20, y_offset),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, text_color, 2)
        else:
            cv2.putText(frame, "Absolute Speed (km/h):", (20, y_offset),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, text_color, 2)
        
        y_offset += 25
        speed_info = []
        
        # æ˜¾ç¤ºæ¯ä¸ªç±»åˆ«çš„å¹³å‡é€Ÿåº¦
        for class_label, speeds in self.track_stats['track_speeds_relative'].items():
            if speeds and self.enable_camera_compensation:
                avg_speed = np.mean(speeds[-5:]) * 3.6  # æœ€è¿‘5å¸§çš„å¹³å‡ç›¸å¯¹é€Ÿåº¦
                speed_info.append(f"{class_label}: {avg_speed:.1f}")
            elif speeds:
                avg_speed = np.mean(speeds[-5:]) * 3.6  # æœ€è¿‘5å¸§çš„å¹³å‡ç»å¯¹é€Ÿåº¦
                speed_info.append(f"{class_label}: {avg_speed:.1f}")
        
        if speed_info:
            speed_text = " | ".join(speed_info)
        else:
            speed_text = "Waiting for tracking data..."
        
        cv2.putText(frame, speed_text, (20, y_offset),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)
        
        return frame
    
    def detect_and_track_video(self, video_path, save_output=True):
        """æ£€æµ‹å¹¶è¿½è¸ªè§†é¢‘ä¸­çš„å¯¹è±¡"""
        if not os.path.exists(video_path):
            logger.error(f"è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
            return None, None
        
        logger.info(f"å¼€å§‹å¤„ç†è§†é¢‘: {os.path.basename(video_path)}")
        
        cap = cv2.VideoCapture(video_path)
        
        if not cap.isOpened():
            logger.error(f"æ— æ³•æ‰“å¼€è§†é¢‘: {video_path}")
            return None, None
        
        fps = cap.get(cv2.CAP_PROP_FPS)
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        frame_area = width * height
        
        logger.info(f"è§†é¢‘ä¿¡æ¯: {width}x{height}, {fps:.2f}fps, {total_frames}å¸§")
        
        video_name = Path(video_path).stem
        output_path = os.path.join(
            self.output_dir,
            f"{video_name}_tracked_congestion.mp4"
        )
        
        if save_output:
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
            
            if not out.isOpened():
                logger.error("æ— æ³•åˆ›å»ºè¾“å‡ºè§†é¢‘å†™å…¥å™¨")
                cap.release()
                return None, None
        
        self.track_stats = {
            'total_frames': 0,
            'unique_pedestrians': set(),
            'unique_bicycles': set(),
            'unique_cars': set(),
            'unique_buses': set(),
            'track_history': defaultdict(list),
            'track_speeds_absolute': defaultdict(list),
            'track_speeds_relative': defaultdict(list),
            'frame_detections': [],
            'max_speeds_absolute': defaultdict(float),
            'max_speeds_relative': defaultdict(float),
            'avg_speeds_absolute': defaultdict(list),
            'avg_speeds_relative': defaultdict(list),
            'congestion_history': [],
        }
        
        self.camera_stats = {
            'motions': [],
            'total_motion': (0, 0),
        }
        
        frame_count = 0
        
        logger.info("å¼€å§‹æ£€æµ‹å’Œè¿½è¸ª...")
        if self.enable_camera_compensation:
            logger.info("âœ… æ‘„åƒå¤´è¿åŠ¨è¡¥å¿å·²å¯ç”¨")
        if self.enable_congestion_detection:
            logger.info("âœ… æ‹¥å µæ£€æµ‹å·²å¯ç”¨")
        if self.use_chinese_text:
            logger.info("âœ… ä½¿ç”¨ä¸­æ–‡æ˜¾ç¤º")
        else:
            logger.info("âœ… ä½¿ç”¨è‹±æ–‡æ˜¾ç¤º")
        
        with tqdm(total=total_frames, desc="å¤„ç†è¿›åº¦") as pbar:
            while True:
                ret, frame = cap.read()
                
                if not ret:
                    break
                
                # æ£€æµ‹æ‘„åƒå¤´è¿åŠ¨
                camera_motion = self.camera_motion_detector.detect_motion(frame)
                self.camera_stats['motions'].append(camera_motion)
                
                # YOLOv8æ£€æµ‹ + ByteTrackè¿½è¸ª
                try:
                    results = self.model.track(
                        frame,
                        conf=self.conf_threshold,
                        iou=self.iou_threshold,
                        tracker=self.tracker_type,
                        verbose=False,
                        persist=True
                    )
                except Exception as e:
                    logger.warning(f"è¿½è¸ªå¤„ç†å¤±è´¥: {e}")
                    # å¦‚æœè¿½è¸ªå¤±è´¥ï¼Œä½¿ç”¨æ™®é€šæ£€æµ‹
                    results = self.model(
                        frame,
                        conf=self.conf_threshold,
                        iou=self.iou_threshold,
                        verbose=False
                    )
                
                # ç»˜åˆ¶æ£€æµ‹ç»“æœ
                frame, frame_stats = self.draw_detections_with_tracking(
                    frame, results, fps, camera_motion
                )
                
                # æ›´æ–°ç»Ÿè®¡
                self.track_stats['total_frames'] += 1
                
                self.track_stats['frame_detections'].append({
                    'frame': frame_count,
                    'detections': frame_stats['detections'],
                    'camera_motion': camera_motion,
                })
                
                # æ”¶é›†é€Ÿåº¦æ•°æ®ç”¨äºæ‹¥å µæ£€æµ‹
                speeds_for_congestion = []
                for track_id in list(self.track_stats['track_speeds_relative'].keys()):
                    speeds = self.track_stats['track_speeds_relative'][track_id]
                    if speeds:
                        speeds_for_congestion.extend(speeds[-5:])  # æœ€è¿‘5ä¸ªé€Ÿåº¦
                
                # æ£€æµ‹æ‹¥å µç­‰çº§
                current_object_count = (len(frame_stats['pedestrian']) + 
                                       len(frame_stats['bicycle']) +
                                       len(frame_stats['car']) +
                                       len(frame_stats['bus']))
                
                congestion_level, congestion_details = self.congestion_detector.detect_congestion_level(
                    speeds_for_congestion, 
                    current_object_count,
                    frame_area
                )
                
                self.track_stats['congestion_history'].append({
                    'frame': frame_count,
                    'level': congestion_level.value,
                    'details': {k: v for k, v in congestion_details.items() if k != 'congestion_factors'}
                })
                
                # ç»˜åˆ¶ç»Ÿè®¡å’Œæ‹¥å µä¿¡æ¯
                unique_stats = {
                    'pedestrians': self.track_stats['unique_pedestrians'],
                    'bicycles': self.track_stats['unique_bicycles'],
                    'cars': self.track_stats['unique_cars'],
                    'buses': self.track_stats['unique_buses'],
                }
                frame = self.draw_statistics(frame, frame_stats, unique_stats, frame_count, fps, 
                                            camera_motion, congestion_level, congestion_details)
                
                # å†™å…¥è¾“å‡ºè§†é¢‘
                if save_output:
                    out.write(frame)
                
                frame_count += 1
                pbar.update(1)
        
        # é‡Šæ”¾èµ„æº
        cap.release()
        if save_output:
            out.release()
        
        logger.info(f"âœ… å¤„ç†å®Œæˆ: {output_path}")
        
        return self.track_stats, output_path
    
    def save_statistics(self, stats, video_name):
        """ä¿å­˜ç»Ÿè®¡ç»“æœ"""
        stats_file = os.path.join(
            self.output_dir,
            'stats',
            f"{video_name}_stats.json"
        )
        
        # è®¡ç®—æ¯ä¸ªå¯¹è±¡çš„å¹³å‡é€Ÿåº¦
        object_speeds = {}
        
        for track_id in list(stats['avg_speeds_relative'].keys()) + list(stats['avg_speeds_absolute'].keys()):
            if track_id in stats['avg_speeds_relative'] and stats['avg_speeds_relative'][track_id]:
                speeds_relative = stats['avg_speeds_relative'][track_id]
                avg_speed_ms_relative = np.mean(speeds_relative)
                max_speed_ms_relative = np.max(speeds_relative)
            else:
                avg_speed_ms_relative = 0
                max_speed_ms_relative = 0
            
            if track_id in stats['avg_speeds_absolute'] and stats['avg_speeds_absolute'][track_id]:
                speeds_absolute = stats['avg_speeds_absolute'][track_id]
                avg_speed_ms_absolute = np.mean(speeds_absolute)
                max_speed_ms_absolute = np.max(speeds_absolute)
            else:
                avg_speed_ms_absolute = 0
                max_speed_ms_absolute = 0
            
            obj_type = 'unknown'
            if track_id in stats['unique_pedestrians']:
                obj_type = 'pedestrian'
            elif track_id in stats['unique_bicycles']:
                obj_type = 'bicycle'
            elif track_id in stats['unique_cars']:
                obj_type = 'car'
            elif track_id in stats['unique_buses']:
                obj_type = 'bus'
            
            if avg_speed_ms_relative > 0 or avg_speed_ms_absolute > 0:
                object_speeds[f"{obj_type}_ID_{track_id}"] = {
                    'type': obj_type,
                    'track_id': track_id,
                    'absolute_speed': {
                        'avg_ms': round(avg_speed_ms_absolute, 3),
                        'avg_kmh': round(avg_speed_ms_absolute * 3.6, 2),
                        'max_ms': round(max_speed_ms_absolute, 3),
                        'max_kmh': round(max_speed_ms_absolute * 3.6, 2),
                    },
                    'relative_speed': {
                        'avg_ms': round(avg_speed_ms_relative, 3),
                        'avg_kmh': round(avg_speed_ms_relative * 3.6, 2),
                        'max_ms': round(max_speed_ms_relative, 3),
                        'max_kmh': round(max_speed_ms_relative * 3.6, 2),
                    },
                    'frames_tracked': len(stats['avg_speeds_relative'].get(track_id, [])),
                }
        
        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å¹³å‡é€Ÿåº¦
        category_speeds = {}
        for class_type in ['pedestrian', 'bicycle', 'car', 'bus']:
            all_speeds_relative = []
            all_speeds_absolute = []
            track_ids = getattr(stats, f'unique_{class_type}s', set())
            
            for track_id in track_ids:
                if track_id in stats['avg_speeds_relative'] and stats['avg_speeds_relative'][track_id]:
                    all_speeds_relative.extend(stats['avg_speeds_relative'][track_id])
                if track_id in stats['avg_speeds_absolute'] and stats['avg_speeds_absolute'][track_id]:
                    all_speeds_absolute.extend(stats['avg_speeds_absolute'][track_id])
            
            if all_speeds_relative or all_speeds_absolute:
                category_speeds[class_type] = {
                    'count': len(track_ids),
                    'absolute_speed': {
                        'avg_ms': round(np.mean(all_speeds_absolute), 3) if all_speeds_absolute else 0,
                        'avg_kmh': round(np.mean(all_speeds_absolute) * 3.6, 2) if all_speeds_absolute else 0,
                        'max_ms': round(np.max(all_speeds_absolute), 3) if all_speeds_absolute else 0,
                        'max_kmh': round(np.max(all_speeds_absolute) * 3.6, 2) if all_speeds_absolute else 0,
                    },
                    'relative_speed': {
                        'avg_ms': round(np.mean(all_speeds_relative), 3) if all_speeds_relative else 0,
                        'avg_kmh': round(np.mean(all_speeds_relative) * 3.6, 2) if all_speeds_relative else 0,
                        'max_ms': round(np.max(all_speeds_relative), 3) if all_speeds_relative else 0,
                        'max_kmh': round(np.max(all_speeds_relative) * 3.6, 2) if all_speeds_relative else 0,
                    }
                }
        
        # è®¡ç®—æ‘„åƒå¤´è¿åŠ¨ç»Ÿè®¡
        if self.camera_stats['motions']:
            camera_motions_pixels = [math.sqrt(m[0]**2 + m[1]**2) for m in self.camera_stats['motions']]
            avg_camera_motion_pixels = np.mean(camera_motions_pixels)
            max_camera_motion_pixels = np.max(camera_motions_pixels)
            
            avg_camera_speed_ms, avg_camera_speed_kmh = self.calculate_speed(
                avg_camera_motion_pixels, len(self.camera_stats['motions']), self.pixels_per_meter
            )
            max_camera_speed_ms, max_camera_speed_kmh = self.calculate_speed(
                max_camera_motion_pixels, len(self.camera_stats['motions']), self.pixels_per_meter
            )
            
            camera_speed_stats = {
                'avg_speed_ms': round(avg_camera_speed_ms, 3),
                'avg_speed_kmh': round(avg_camera_speed_kmh, 2),
                'max_speed_ms': round(max_camera_speed_ms, 3),
                'max_speed_kmh': round(max_camera_speed_kmh, 2),
                'has_motion': max_camera_speed_kmh > 0.5,
            }
        else:
            camera_speed_stats = {'has_motion': False}
        
        # è®¡ç®—æ‹¥å µç»Ÿè®¡
        if stats['congestion_history']:
            congestion_counts = defaultdict(int)
            for cong_data in stats['congestion_history']:
                level = cong_data['level']
                congestion_counts[level] += 1
            
            total_congestion_frames = len(stats['congestion_history'])
            congestion_stats = {
                'total_frames_analyzed': total_congestion_frames,
                'level_distribution': {k: v for k, v in congestion_counts.items()},
                'level_percentages': {k: round(100 * v / total_congestion_frames, 2) 
                                      for k, v in congestion_counts.items()},
                'most_common_level': max(congestion_counts, key=congestion_counts.get) if congestion_counts else 'unknown'
            }
        else:
            congestion_stats = {'total_frames_analyzed': 0}
        
        result_stats = {
            'video': video_name,
            'timestamp': datetime.now().isoformat(),
            'total_frames': stats['total_frames'],
            'unique_counts': {
                'pedestrian': len(stats['unique_pedestrians']),
                'bicycle': len(stats['unique_bicycles']),
                'car': len(stats['unique_cars']),
                'bus': len(stats['unique_buses']),
                'total': (len(stats['unique_pedestrians']) + 
                         len(stats['unique_bicycles']) +
                         len(stats['unique_cars']) +
                         len(stats['unique_buses']))
            },
            'speed_statistics': {
                'by_category': category_speeds,
                'by_object': object_speeds,
            },
            'camera_motion': camera_speed_stats,
            'congestion_analysis': congestion_stats,
            'congestion_levels': {
                'smooth': 'ç•…é€š',
                'light': 'è½»å¾®æ‹¥å µ',
                'moderate': 'ä¸­ç­‰æ‹¥å µ',
                'heavy': 'ä¸¥é‡æ‹¥å µ',
                'severe': 'æåº¦æ‹¥å µ',
            },
            'model': self.model_name,
            'confidence_threshold': self.conf_threshold,
            'iou_threshold': self.iou_threshold,
            'tracker': self.tracker_type,
            'camera_compensation': self.enable_camera_compensation,
            'congestion_detection': self.enable_congestion_detection,
            'chinese_text': self.use_chinese_text,
        }
        
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(result_stats, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ç»Ÿè®¡ç»“æœå·²ä¿å­˜: {stats_file}")
        
        return stats_file
    
    def print_statistics(self, stats, video_name):
        """æ‰“å°ç»Ÿè®¡ç»“æœ"""
        print("\n" + "=" * 100)
        print(f"ğŸ“Š æ£€æµ‹è¿½è¸ªç»Ÿè®¡ç»“æœ - {video_name}")
        print("=" * 100)
        print(f"æ€»å¸§æ•°: {stats['total_frames']}")
        
        print(f"\nğŸš¶ è¡Œäºº (Pedestrian):")
        print(f"   å”¯ä¸€IDæ•°: {len(stats['unique_pedestrians'])}")
        
        print(f"\nğŸš´ è‡ªè¡Œè½¦ (Bicycle):")
        print(f"   å”¯ä¸€IDæ•°: {len(stats['unique_bicycles'])}")
        
        print(f"\nğŸš— å°è½¿è½¦ (Car):")
        print(f"   å”¯ä¸€IDæ•°: {len(stats['unique_cars'])}")
        
        print(f"\nğŸšŒ å…¬å…±æ±½è½¦ (Bus):")
        print(f"   å”¯ä¸€IDæ•°: {len(stats['unique_buses'])}")
        
        # æ‘„åƒå¤´è¿åŠ¨ç»Ÿè®¡
        if self.camera_stats['motions']:
            camera_motions_pixels = [math.sqrt(m[0]**2 + m[1]**2) for m in self.camera_stats['motions']]
            avg_motion = np.mean(camera_motions_pixels)
            avg_speed_ms, avg_speed_kmh = self.calculate_speed(
                avg_motion, len(self.camera_stats['motions']), self.pixels_per_meter
            )
            print(f"\nğŸ“¹ æ‘„åƒå¤´è¿åŠ¨ç»Ÿè®¡:")
            print(f"   å¹³å‡è¿åŠ¨é€Ÿåº¦: {avg_speed_kmh:.1f} km/h")
        
        # æ‹¥å µç»Ÿè®¡
        if stats['congestion_history']:
            congestion_counts = defaultdict(int)
            for cong_data in stats['congestion_history']:
                level = cong_data['level']
                congestion_counts[level] += 1
            
            print(f"\nğŸš¦ é“è·¯æ‹¥å µåˆ†æ:")
            level_names = {
                'smooth': 'ç•…é€š âœ…',
                'light': 'è½»å¾®æ‹¥å µ âš ï¸',
                'moderate': 'ä¸­ç­‰æ‹¥å µ âš ï¸âš ï¸',
                'heavy': 'ä¸¥é‡æ‹¥å µ ğŸ”´',
                'severe': 'æåº¦æ‹¥å µ ğŸ”´ğŸ”´',
            }
            
            for level, count in sorted(congestion_counts.items()):
                percentage = 100 * count / len(stats['congestion_history'])
                level_name = level_names.get(level, level)
                print(f"   {level_name}: {count}å¸§ ({percentage:.1f}%)")
         
        total = (len(stats['unique_pedestrians']) + 
                len(stats['unique_bicycles']) +
                len(stats['unique_cars']) +
                len(stats['unique_buses']))
        print(f"\nâœ… æ€»å”¯ä¸€å¯¹è±¡æ•°: {total}")
        print("=" * 100 + "\n")
    
    def process_directory(self, directory=None, extensions=['.mp4', '.avi', '.mov', '.mkv']):
        """æ‰¹é‡å¤„ç†ç›®å½•ä¸­çš„æ‰€æœ‰è§†é¢‘"""
        if directory is None:
            directory = self.input_dir
        
        video_files = []
        for ext in extensions:
            video_files.extend(Path(directory).glob(f"*{ext}"))
            video_files.extend(Path(directory).glob(f"*{ext.upper()}"))
        
        if not video_files:
            logger.warning(f"æœªæ‰¾åˆ°è§†é¢‘æ–‡ä»¶: {directory}")
            return []
        
        logger.info(f"æ‰¾åˆ° {len(video_files)} ä¸ªè§†é¢‘æ–‡ä»¶")
        
        results = []
        for idx, video_path in enumerate(video_files, 1):
            logger.info(f"\n[{idx}/{len(video_files)}] å¤„ç†: {video_path.name}")
            try:
                stats, output_path = self.detect_and_track_video(str(video_path))
                
                if stats and output_path:
                    self.print_statistics(stats, video_path.stem)
                    self.save_statistics(stats, video_path.stem)
                    
                    results.append({
                        'input': str(video_path),
                        'output': output_path,
                        'stats': stats
                    })
            except Exception as e:
                logger.error(f"å¤„ç†è§†é¢‘å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
        
        logger.info(f"\nâœ… æ€»å…±æˆåŠŸå¤„ç† {len(results)}/{len(video_files)} ä¸ªè§†é¢‘")
        
        return results
    
    def generate_summary_report(self, results):
        """ç”Ÿæˆæ€»ç»“æŠ¥å‘Š"""
        if not results:
            logger.warning("æ²¡æœ‰å¤„ç†ç»“æœ")
            return
        
        report_file = os.path.join(self.output_dir, 'detection_summary.json')
        
        summary = {
            'timestamp': datetime.now().isoformat(),
            'total_videos': len(results),
            'tracking_method': self.tracker_type,
            'model': self.model_name,
            'camera_compensation_enabled': self.enable_camera_compensation,
            'congestion_detection_enabled': self.enable_congestion_detection,
            'chinese_text_enabled': self.use_chinese_text,
            'total_unique_counts': {
                'pedestrian': sum(len(r['stats']['unique_pedestrians']) for r in results),
                'bicycle': sum(len(r['stats']['unique_bicycles']) for r in results),
                'car': sum(len(r['stats']['unique_cars']) for r in results),
                'bus': sum(len(r['stats']['unique_buses']) for r in results),
            },
            'videos': []
        }
        
        for result in results:
            summary['videos'].append({
                'name': Path(result['input']).stem,
                'input': result['input'],
                'output': result['output'],
                'unique_counts': {
                    'pedestrian': len(result['stats']['unique_pedestrians']),
                    'bicycle': len(result['stats']['unique_bicycles']),
                    'car': len(result['stats']['unique_cars']),
                    'bus': len(result['stats']['unique_buses']),
                }
            })
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        logger.info(f"æ€»ç»“æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        
        print("\n" + "=" * 100)
        print("ğŸ“ˆ æ•´ä½“è¿½è¸ªæ€»ç»“")
        print("=" * 100)
        print(f"å¤„ç†è§†é¢‘æ•°: {summary['total_videos']}")
        print(f"è¿½è¸ªæ–¹æ³•: {summary['tracking_method']}")
        print(f"æ‘„åƒå¤´è¿åŠ¨è¡¥å¿: {'âœ… å¯ç”¨' if summary['camera_compensation_enabled'] else 'âŒ ç¦ç”¨'}")
        print(f"æ‹¥å µæ£€æµ‹: {'âœ… å¯ç”¨' if summary['congestion_detection_enabled'] else 'âŒ ç¦ç”¨'}")
        print(f"ä¸­æ–‡æ˜¾ç¤º: {'âœ… å¯ç”¨' if summary['chinese_text_enabled'] else 'âŒ ç¦ç”¨'}")
        print(f"\nå…¨éƒ¨è§†é¢‘ç´¯è®¡å”¯ä¸€å¯¹è±¡æ•°:")
        print(f"  è¡Œäºº: {summary['total_unique_counts']['pedestrian']}")
        print(f"  è‡ªè¡Œè½¦: {summary['total_unique_counts']['bicycle']}")
        print(f"  å°è½¿è½¦: {summary['total_unique_counts']['car']}")
        print(f"  å…¬å…±æ±½è½¦: {summary['total_unique_counts']['bus']}")
        
        total = sum(summary['total_unique_counts'].values())
        print(f"\næ€»è®¡: {total}")
        print("=" * 100 + "\n")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description='YOLOv8 + ByteTrack æ£€æµ‹è¿½è¸ªå·¥å…·ï¼ˆå¸¦æ‹¥å µæ£€æµ‹ï¼‰'
    )
    parser.add_argument('input', nargs='?', default='pre_video',
                       help='è¾“å…¥è§†é¢‘æ–‡ä»¶æˆ–ç›®å½•')
    parser.add_argument('-o', '--output', default='output_video_2',
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('-m', '--model', default='yolov8l.pt',
                       help='YOLOv8æ¨¡å‹')
    parser.add_argument('-c', '--conf', type=float, default=0.3,
                       help='ç½®ä¿¡åº¦é˜ˆå€¼')
    parser.add_argument('--iou', type=float, default=0.5,
                       help='NMS IOUé˜ˆå€¼')
    parser.add_argument('-t', '--tracker', default='bytetrack.yaml',
                       help='è¿½è¸ªå™¨ç±»å‹é…ç½®æ–‡ä»¶')
    parser.add_argument('--ppm', type=float, default=50,
                       help='åƒç´ ä¸ç±³çš„è½¬æ¢æ¯”ä¾‹')
    parser.add_argument('--no-camera-compensation', action='store_true',
                       help='ç¦ç”¨æ‘„åƒå¤´è¿åŠ¨è¡¥å¿')
    parser.add_argument('--no-congestion-detection', action='store_true',
                       help='ç¦ç”¨æ‹¥å µæ£€æµ‹')
    parser.add_argument('--no-chinese', action='store_true',
                       help='ç¦ç”¨ä¸­æ–‡æ˜¾ç¤ºï¼ˆä½¿ç”¨è‹±æ–‡ï¼‰')
    
    args = parser.parse_args()
    
    detector = YOLOTrackerDetector(
        model_name=args.model,
        conf_threshold=args.conf,
        iou_threshold=args.iou,
        output_dir=args.output,
        tracker=args.tracker,
        pixels_per_meter=args.ppm,
        enable_camera_compensation=not args.no_camera_compensation,
        enable_congestion_detection=not args.no_congestion_detection,
        use_chinese_text=not args.no_chinese
    )
    
    if os.path.isfile(args.input):
        logger.info(f"å¤„ç†å•ä¸ªè§†é¢‘æ–‡ä»¶: {args.input}")
        stats, output_path = detector.detect_and_track_video(args.input)
        if stats:
            detector.print_statistics(stats, Path(args.input).stem)
            detector.save_statistics(stats, Path(args.input).stem)
    elif os.path.isdir(args.input):
        logger.info(f"æ‰¹é‡å¤„ç†ç›®å½•: {args.input}")
        results = detector.process_directory(args.input)
        if results:
            detector.generate_summary_report(results)
    else:
        logger.error(f"æ— æ•ˆçš„è¾“å…¥è·¯å¾„: {args.input}")


if __name__ == "__main__":
    main()
